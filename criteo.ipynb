{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Criteo Clickthrough Data\n",
    "We will implement a neural network to predict ad clickthrough probabilities with Keras. \n",
    "We start by importing some modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network consists of 3 dense hidden layers, each with 500 neurons, and a single-neuron final layer with a sigmoid activation function, representing the probability that the ad is clicked. We stack them together using `keras.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1 = keras.layers.Dense(500, activation=tf.nn.relu)\n",
    "hidden_layer_2 = keras.layers.Dense(500, activation=tf.nn.relu)\n",
    "hidden_layer_3 = keras.layers.Dense(500, activation=tf.nn.relu)\n",
    "final_layer = keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "\n",
    "layers = keras.Sequential([\n",
    "    hidden_layer_1,\n",
    "    hidden_layer_2,\n",
    "    hidden_layer_3,\n",
    "    final_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will be trained with binary cross entropy loss since the dataset has two classes (1 = clicked, 0 = not clicked). For the optimizer, we will use ADAM. To evaluate the model, we use the accuracy metric, which simply shows us how often the model predicts correctly (probability > 0.5 when truth = 1, and probability < 0.5 when truth = 0).\n",
    "\n",
    "We are not setting `from_logits=True` in the loss function since the final layer has a sigmoid activation; it is already a probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "optimizer = keras.optimizers.Adam()\n",
    "metric = keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is almost done! However, we still haven't prepared our datasets. We need to prepare it in a way specific to the Criteo dataset. \n",
    "Specifically, it has 13 integer features (count features) and 26 categorical features hashed to 32 bits. while we can stack the 13 integer features into a vector, the categorical features must be treated differently since it does not make sense to treat category labels as scalars (e.g. if category car = 1 and category apple = 2, it does not make sense that apple is 2 x car). At the same time, each feature may comprise too large of a vocabulary to be one-hot encoded. Thus, we will use embedding tables, one for each categorical feature. The dimensions of each feature is not readily available, so we need to analyze the dataset to find out. For now, we use the small Criteo dataset with only 1,000,000 entries.\n",
    "\n",
    "Here are the dimensions of each data point\n",
    "\n",
    "0 : 1261    | 1 : 531       | 2 : 321438    | 3 : 120964    | 4 : 267   | 5 : 15        | 6 : 10863 | 7 : 563   | 8 : 3     | 9 : 30792 \n",
    "\n",
    "10 : 4731   | 11 : 268487   | 12 : 3068     | 13 : 26       | 14 : 8934 | 15 : 205923   | 16 : 10   | 17 : 3881 | 18 : 1854 | 19 : 3\n",
    "\n",
    "20 : 240747 | 21 : 15       | 22 : 15       | 23 : 41282    | 24 : 69   | 25 : 30956\n",
    "\n",
    "Since some of them have very small dimensions, we will only use embeddings for features with > 100 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layers = [\n",
    "    keras.layers.Embedding(input_dim=100, output_dim=70, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=531, output_dim=50, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=600, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=400, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=267, output_dim=50, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=15, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=110, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "                            \n",
    "    keras.layers.Embedding(input_dim=563, output_dim=50, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=3, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=200, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=200, output_dim=70, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=600, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=150, output_dim=70, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=26, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=300, output_dim=70, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=500, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=10, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=250, output_dim=70, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=100, output_dim=70, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=3, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=600, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=15, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=15, output_mode='one_hot'),\n",
    "\n",
    "    keras.layers.Embedding(input_dim=200, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "\n",
    "    keras.layers.CategoryEncoding(num_tokens=69, output_mode='one_hot'),\n",
    "    \n",
    "    keras.layers.Embedding(input_dim=200, output_dim=100, \n",
    "                            embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=1.)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_helper(layer, inputs, index):\n",
    "    layer_inputs = tf.gather(inputs, [index], axis=1)\n",
    "    if isinstance(layer, tf.keras.layers.Embedding):\n",
    "        layer_inputs = tf.math.mod(layer_inputs, layer.input_dim)\n",
    "        output_dim = layer.output_dim\n",
    "    else:\n",
    "        layer_inputs = tf.math.mod(layer_inputs, layer.num_tokens)\n",
    "        output_dim = layer.num_tokens\n",
    "    final_shape = (-1, output_dim)\n",
    "    return tf.reshape(layer(layer_inputs), final_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine everything into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(39,), dtype=tf.float32)\n",
    "x = keras.layers.Concatenate()([\n",
    "    tf.gather(inputs, range(0,13), axis=1),\n",
    "    *[concat_helper(layer, inputs, i + 13) for i, layer in enumerate(embedding_layers)]\n",
    "])\n",
    "\n",
    "outputs = layers(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=loss_fn, optimizer=optimizer, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the experiments, we need to first parse the dataset and package it as a tensorflow dataset. First, we load everything into memory as numpy array, then we will cast it as a tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Users/benitogeordie/Desktop/thirdai_datasets/criteo/kaggleAdDisplayChallenge_processed.npz')\n",
    "\n",
    "X_cat = data['X_cat'].astype(np.int32)\n",
    "X_int = data['X_int'].astype(np.int32)\n",
    "y = data['y']\n",
    "counts = data['counts']\n",
    "\n",
    "start_idx = np.zeros(len(counts)+1, dtype=np.int32)\n",
    "start_idx[1:] = np.cumsum(counts)\n",
    "\n",
    "idxs = np.arange(y.shape[0])\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "n_train = int(len(idxs)*0.8)\n",
    "n_test = y.shape[0]-n_train\n",
    "\n",
    "train_idxs = idxs[:n_train]\n",
    "test_idxs = idxs[n_train:]\n",
    "\n",
    "X_cat_train = X_cat[train_idxs]\n",
    "X_cat_test = X_cat[test_idxs]\n",
    "\n",
    "X_int_train = X_int[train_idxs]\n",
    "X_int_test = X_int[test_idxs]\n",
    "\n",
    "y_train = y[train_idxs]\n",
    "y_test = y[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((X_int_train, X_cat_train), axis=1)\n",
    "x_test = np.concatenate((X_int_test, X_cat_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = '/Users/benitogeordie/Desktop/thirdai_datasets/criteo/train_shuf.txt' # TODO: Always make sure this is correct before running\n",
    "# test_path = '/Users/benitogeordie/Desktop/thirdai_datasets/criteo/test_shuf.txt' # TODO: Always make sure this is correct before running\n",
    "\n",
    "# def load_examples_and_labels(criteo_path):\n",
    "#     examples = np.ndarray([0,39], dtype=np.int)\n",
    "#     labels = np.ndarray([0,1], dtype=np.int)\n",
    "\n",
    "#     f = open(criteo_path)\n",
    "\n",
    "#     for line in f:\n",
    "#         itms = line.split(' ')\n",
    "#         np.append(labels, [[np.int32(itms[0])]], axis=0)\n",
    "#         np.append(examples, [np.int32(itm) if itm!='' else 0 for itm in itms[1:]])\n",
    "\n",
    "#     return (examples, labels)\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(load_examples_and_labels(train_path))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices(load_examples_and_labels(test_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data is unbalanced, we need to check the distribution of positive vs negative examples. Suppose 75% of the examples are negative. Then even if we just predicted false for everything, we would get 75% accuracy. Thus, even a 70% accuracy is not good. We want at least 75% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 74.37850761877574% negative.\n",
      "Test: 74.37427766029343% negative.\n"
     ]
    }
   ],
   "source": [
    "def get_percent_neg(labels):\n",
    "    n_examples = labels.shape[0]\n",
    "    negatives = n_examples - np.count_nonzero(labels)\n",
    "    percent_negative = 100 * negatives / n_examples\n",
    "    return percent_negative\n",
    "\n",
    "print(f\"Train: {get_percent_neg(y_train)}% negative.\")\n",
    "print(f\"Test: {get_percent_neg(y_test)}% negative.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets give her a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256\n",
    "# train_batches = train_dataset.batch(batch_size)\n",
    "# test_batches = test_dataset.batch(batch_size)\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(f\"Epoch {i + 1}/10\")\n",
    "#     model.fit(train_batches)\n",
    "#     model.evaluate(test_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we instead diversity-sampled the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "0.22\n"
     ]
    }
   ],
   "source": [
    "class MinHash:\n",
    "    def __init__(self, r_repetitions: int, h_hashes_per_table: int, b_buckets: int, seed: int=314152):\n",
    "        num_hashes = r_repetitions * h_hashes_per_table\n",
    "        g = tf.random.Generator.from_seed(seed)\n",
    "        self.a = g.uniform(shape=(1, num_hashes), dtype=tf.int64, minval=None)\n",
    "        self.b = g.uniform(shape=(num_hashes,), dtype=tf.int64, minval=None)\n",
    "\n",
    "        self.b_buckets = b_buckets\n",
    "        self.table_shape = (r_repetitions, h_hashes_per_table)\n",
    "        self.r_start_idxs = tf.constant([i * b_buckets for i in range(r_repetitions)], dtype=tf.int64)\n",
    "    \n",
    "    @tf.function\n",
    "    def hash(self, tensor: tf.Tensor):\n",
    "        vertical_tensor = tf.reshape(tensor, (-1, 1))\n",
    "        hashes = (tf.matmul(vertical_tensor, self.a) + self.b) % self.b_buckets\n",
    "        hashes = tf.reduce_min(hashes, axis=0)\n",
    "        hashes = tf.reshape(hashes, self.table_shape)\n",
    "        hashes = tf.as_string(hashes)\n",
    "        hashes = tf.strings.reduce_join(hashes, axis=-1)\n",
    "        return tf.cast(tf.strings.to_hash_bucket_fast(hashes, self.b_buckets), dtype=tf.int64) + self.r_start_idxs\n",
    "    \n",
    "    def summary(self):\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Race:\n",
    "    def __init__(self, r_repetitions: int, b_buckets: int, h_hashes_per_table: int):\n",
    "        self.arrays = tf.Variable(np.zeros(shape=(r_repetitions * b_buckets)), dtype=tf.float64)\n",
    "        self.hash = MinHash(r_repetitions, h_hashes_per_table, b_buckets).hash #tf.function()\n",
    "\n",
    "    @tf.function\n",
    "    def query(self, tensor: tf.Tensor):\n",
    "        hashes = self.hash(tensor)\n",
    "        return tf.reduce_mean(tf.gather(self.arrays, hashes))\n",
    "\n",
    "    @tf.function\n",
    "    def index(self, tensor: tf.Tensor):\n",
    "        hashes = tf.reshape(self.hash(tensor), (-1, 1))\n",
    "        self.arrays.assign(tf.tensor_scatter_nd_add(self.arrays, hashes, tf.ones(shape=hashes.shape[0], dtype=tf.float64)))\n",
    "\n",
    "    @tf.function\n",
    "    def index_and_query(self, tensor: tf.Tensor):\n",
    "        hashes = tf.reshape(self.hash(tensor), (-1, 1))\n",
    "        self.arrays.assign(tf.tensor_scatter_nd_add(self.arrays, hashes, tf.ones(shape=hashes.shape[0], dtype=tf.float64)))\n",
    "        return tf.reduce_mean(tf.gather(self.arrays, hashes))\n",
    "    \n",
    "    @tf.function\n",
    "    def query_and_index(self, tensor: tf.Tensor):\n",
    "        hashes = tf.reshape(self.hash(tensor), (-1, 1))\n",
    "        result = tf.reduce_mean(tf.gather(self.arrays, hashes))\n",
    "        self.arrays.assign(tf.tensor_scatter_nd_add(self.arrays, hashes, tf.ones(shape=hashes.shape[0], dtype=tf.float64)))\n",
    "        return result\n",
    "    \n",
    "    def summary(self):\n",
    "        # Mean\n",
    "        # Stdev\n",
    "        # Num zeros\n",
    "        # Nonzero min\n",
    "        # Max\n",
    "        return\n",
    "\n",
    "race = Race(10, 1000, 2)\n",
    "tensor1 = tf.constant([1,2,3,8], dtype=tf.int64)\n",
    "tensor2 = tf.constant([7,2,3,8], dtype=tf.int64)\n",
    "# index = tf.function(race.index)\n",
    "# query = tf.function(race.query)\n",
    "race.index(tensor1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.8, shape=(), dtype=float64)\n",
      "tf.Tensor(8.6, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "race.index(tensor1)\n",
    "race.index(tensor1)\n",
    "race.index(tensor1)\n",
    "race.index(tensor1)\n",
    "race.index(tensor2)\n",
    "race.index(tensor2)\n",
    "race.index(tensor2)\n",
    "race.index(tensor2)\n",
    "race.index(tensor2)\n",
    "race.index(tensor2)\n",
    "race.index(tensor2)\n",
    "print(race.query(tensor1))\n",
    "print(race.query(tensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the continuous features\n",
    "# Make the mapping function\n",
    "def quantize(columns: np.array, bin_widths: np.array, n_bins: int):\n",
    "    # TODO: Should I allow arrays of bin_widths and n_binss because each feature has a different range?\n",
    "    \"\"\"\n",
    "    Quantize into bins but keep some notion of locality sensitivity\n",
    "    \"\"\"\n",
    "    assert(len(columns.shape) == 2)\n",
    "    n_cols = columns.shape[1]\n",
    "    new_arr = np.reshape(columns, columns.shape + (1,))\n",
    "    new_arr = np.repeat(new_arr, n_bins, axis=2)\n",
    "    \n",
    "    bin_idxs = np.arange(n_bins)\n",
    "    add = np.reshape(bin_idxs, (1, n_bins)) * np.reshape(bin_widths, (n_cols, 1)) // n_bins\n",
    "    new_arr = (new_arr + add)\n",
    "    new_arr = new_arr // np.repeat(np.reshape(bin_widths, (n_cols, 1)), n_bins, axis=1)\n",
    "    new_arr[new_arr < 0] = 0\n",
    "    return tf.reshape(new_arr, (-1, columns.shape[1] * n_bins))\n",
    "\n",
    "def separate_domains(columns: np.array):\n",
    "    \"\"\"\n",
    "    Separate domains by interleaving them (instead of shifting by domain ranges).\n",
    "    This makes it range-agnostic\n",
    "    \"\"\"\n",
    "    assert(len(columns.shape) == 2)\n",
    "    n_domains = columns.shape[1]\n",
    "    idx_shifts = np.arange(n_domains)\n",
    "    return columns * n_domains + idx_shifts\n",
    "\n",
    "# print(quantize(np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]), np.array([2, 2]), 2))\n",
    "# print(separate_domains(quantize(np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]), np.array([2, 2]), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36672493, 13)\n",
      "(13, 195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/94hg7dn12_jctmbpb7wjzjkm0000gn/T/ipykernel_3281/3463918945.py:16: RuntimeWarning: divide by zero encountered in floor_divide\n",
      "  new_arr = new_arr // np.repeat(np.reshape(bin_widths, (n_cols, 1)), n_bins, axis=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 195 and the array at index 1 has size 39",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [151]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m x_train_int_binned \u001b[39m=\u001b[39m quantize(x_train[:\u001b[39m13\u001b[39m], bin_widths\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mmax(x_train, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m, n_bins\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(x_train_int_binned\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m x_train \u001b[39m=\u001b[39m separate_domains(np\u001b[39m.\u001b[39;49mconcatenate((x_train_int_binned, x_train[\u001b[39m13\u001b[39;49m:]), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 195 and the array at index 1 has size 39"
     ]
    }
   ],
   "source": [
    "print(x_train[:,:13].shape)\n",
    "x_train_int_binned = quantize(x_train[:,:13], bin_widths=np.max(x_train, axis=0) // 100, n_bins=5)\n",
    "print(x_train_int_binned.shape)\n",
    "x_train = separate_domains(np.concatenate((x_train_int_binned, x_train[13:]), axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "Consider:\n",
    "- Reweighting vs no reweighting vs variable reweighting\n",
    "- Even distribution across clusters vs prioritizing hard samples (use for max-likelihood)\n",
    "\n",
    "\n",
    "- Diversification of results\n",
    "- Prove something about how RACE maximizes a diversity metric\n",
    "- Expert selection\n",
    "- other anomaly detection ideas\n",
    "- race in place of convolution filters? how complex is convolution filter? Instead of multiplying with each filter, we can use race to match with most relevant filters using just a few hash computations, allowing us to do efficient inference with many filters. Allows us to use larger patches or kernels? less convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment should be reproducible by running an executable\n",
    "- It's ok if I write a separate exec for each dataset.\n",
    "- bash script for each exp"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d93aadd6b166e0005467eb9404d616711328a74af59027aefe10194cc2eaa784"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
